# ğŸ›’ æ²ƒå°”ç›AI Agentå¹³å° - æ•°æ®åˆ†æAgent
# Walmart AI Agent Platform - Data Analysis Agent

import json
import logging
from typing import Any, Dict, List, Optional
from datetime import datetime, timedelta
import pandas as pd
import numpy as np

from app.agents.base_agent import BaseAgent, AgentCapability, AgentContext, AgentMessage, AgentTask

logger = logging.getLogger(__name__)


class DataAnalysisAgent(BaseAgent):
    """æ•°æ®åˆ†æAgent - ä¸“é—¨å¤„ç†å¤æ‚æ•°æ®åˆ†æã€ç»Ÿè®¡å»ºæ¨¡å’Œæ•°æ®å¯è§†åŒ–"""
    
    def __init__(self, **kwargs):
        super().__init__(
            name="æ•°æ®åˆ†æä¸“å®¶",
            description="ä¸“é—¨å¤„ç†å¤æ‚æ•°æ®åˆ†æã€ç»Ÿè®¡å»ºæ¨¡ã€é¢„æµ‹åˆ†æå’Œæ•°æ®å¯è§†åŒ–ä»»åŠ¡",
            capabilities=[
                AgentCapability.DATA_ANALYSIS,
                AgentCapability.REASONING,
                AgentCapability.PLANNING,
                AgentCapability.NATURAL_LANGUAGE,
                AgentCapability.MULTI_MODAL
            ],
            **kwargs
        )
        
        # æ”¯æŒçš„åˆ†æç±»å‹
        self.analysis_types = {
            "descriptive": "æè¿°æ€§åˆ†æ",
            "diagnostic": "è¯Šæ–­æ€§åˆ†æ", 
            "predictive": "é¢„æµ‹æ€§åˆ†æ",
            "prescriptive": "æŒ‡å¯¼æ€§åˆ†æ",
            "statistical": "ç»Ÿè®¡åˆ†æ",
            "correlation": "ç›¸å…³æ€§åˆ†æ",
            "regression": "å›å½’åˆ†æ",
            "clustering": "èšç±»åˆ†æ",
            "time_series": "æ—¶é—´åºåˆ—åˆ†æ",
            "cohort": "é˜Ÿåˆ—åˆ†æ"
        }
        
        # å¯è§†åŒ–å›¾è¡¨ç±»å‹
        self.chart_types = [
            "line", "bar", "scatter", "pie", "heatmap", 
            "box", "violin", "histogram", "area", "radar",
            "treemap", "sunburst", "sankey", "funnel", "gauge"
        ]
    
    async def process_message(
        self,
        message: str,
        context: AgentContext,
        **kwargs
    ) -> AgentMessage:
        """å¤„ç†æ•°æ®åˆ†æç›¸å…³æ¶ˆæ¯"""
        
        # è¯†åˆ«åˆ†æéœ€æ±‚
        analysis_request = self._parse_analysis_request(message)
        
        # æœç´¢ç›¸å…³æ•°æ®å’Œåˆ†ææ¡ˆä¾‹
        knowledge = await self.search_knowledge(
            query=message,
            collection_name="data_analysis_kb",
            n_results=5
        )
        
        # æ„å»ºåˆ†ææç¤º
        analysis_prompt = self._build_analysis_prompt(
            message, analysis_request, knowledge, context
        )
        
        # ç”Ÿæˆåˆ†æç»“æœ
        analysis_result = await self.generate_response(
            prompt=analysis_prompt,
            context=context,
            temperature=0.2,  # ä½æ¸©åº¦ç¡®ä¿åˆ†æå‡†ç¡®æ€§
            max_tokens=2500
        )
        
        # ç”Ÿæˆå¯è§†åŒ–å»ºè®®
        viz_suggestions = self._generate_visualization_suggestions(analysis_request)
        
        # ç”Ÿæˆä»£ç ç¤ºä¾‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        code_examples = self._generate_code_examples(analysis_request)
        
        return AgentMessage(
            role="assistant",
            content=analysis_result,
            metadata={
                "analysis_type": analysis_request["type"],
                "data_sources": len(knowledge),
                "visualization_suggestions": viz_suggestions,
                "code_examples": code_examples,
                "complexity_level": analysis_request.get("complexity", "medium")
            }
        )
    
    async def execute_task(
        self,
        task: AgentTask,
        context: AgentContext,
        **kwargs
    ) -> AgentTask:
        """æ‰§è¡Œæ•°æ®åˆ†æä»»åŠ¡"""
        
        task_type = task.metadata.get("task_type", "data_analysis")
        
        try:
            if task_type == "statistical_analysis":
                result = await self._perform_statistical_analysis(task, context)
            elif task_type == "predictive_modeling":
                result = await self._build_predictive_model(task, context)
            elif task_type == "data_exploration":
                result = await self._explore_data(task, context)
            elif task_type == "correlation_analysis":
                result = await self._analyze_correlations(task, context)
            elif task_type == "time_series_analysis":
                result = await self._analyze_time_series(task, context)
            elif task_type == "cohort_analysis":
                result = await self._perform_cohort_analysis(task, context)
            elif task_type == "ab_testing":
                result = await self._analyze_ab_test(task, context)
            elif task_type == "data_quality_check":
                result = await self._check_data_quality(task, context)
            else:
                result = await self._general_data_analysis(task, context)
            
            task.output_data = result
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åˆ†æä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            task.output_data = {
                "error": str(e),
                "task_type": task_type,
                "suggestion": "è¯·æ£€æŸ¥æ•°æ®æ ¼å¼å’Œåˆ†æå‚æ•°"
            }
            raise
        
        return task
    
    def _parse_analysis_request(self, message: str) -> Dict[str, Any]:
        """è§£æåˆ†æè¯·æ±‚"""
        message_lower = message.lower()
        
        # è¯†åˆ«åˆ†æç±»å‹
        analysis_type = "descriptive"  # é»˜è®¤
        
        if any(word in message_lower for word in ["é¢„æµ‹", "é¢„æŠ¥", "forecast", "predict"]):
            analysis_type = "predictive"
        elif any(word in message_lower for word in ["ç›¸å…³", "å…³è”", "correlation"]):
            analysis_type = "correlation"
        elif any(word in message_lower for word in ["å›å½’", "regression"]):
            analysis_type = "regression"
        elif any(word in message_lower for word in ["èšç±»", "åˆ†ç¾¤", "cluster"]):
            analysis_type = "clustering"
        elif any(word in message_lower for word in ["æ—¶é—´åºåˆ—", "time series", "è¶‹åŠ¿"]):
            analysis_type = "time_series"
        elif any(word in message_lower for word in ["é˜Ÿåˆ—", "cohort", "ç”¨æˆ·ç¾¤ä½“"]):
            analysis_type = "cohort"
        elif any(word in message_lower for word in ["ç»Ÿè®¡", "statistical", "æ£€éªŒ"]):
            analysis_type = "statistical"
        elif any(word in message_lower for word in ["ä¸ºä»€ä¹ˆ", "åŸå› ", "è¯Šæ–­"]):
            analysis_type = "diagnostic"
        elif any(word in message_lower for word in ["å»ºè®®", "ä¼˜åŒ–", "æ”¹è¿›"]):
            analysis_type = "prescriptive"
        
        # è¯†åˆ«å¤æ‚åº¦
        complexity = "medium"
        if any(word in message_lower for word in ["ç®€å•", "åŸºç¡€", "æ¦‚è¿°"]):
            complexity = "low"
        elif any(word in message_lower for word in ["æ·±å…¥", "è¯¦ç»†", "é«˜çº§", "å¤æ‚"]):
            complexity = "high"
        
        # è¯†åˆ«æ•°æ®ç±»å‹
        data_types = []
        if any(word in message_lower for word in ["é”€å”®", "è¥æ”¶", "æ”¶å…¥"]):
            data_types.append("sales")
        if any(word in message_lower for word in ["å®¢æˆ·", "ç”¨æˆ·"]):
            data_types.append("customer")
        if any(word in message_lower for word in ["åº“å­˜", "å•†å“"]):
            data_types.append("inventory")
        if any(word in message_lower for word in ["æµé‡", "è®¿é—®"]):
            data_types.append("traffic")
        
        return {
            "type": analysis_type,
            "complexity": complexity,
            "data_types": data_types,
            "original_message": message
        }
    
    def _build_analysis_prompt(
        self,
        message: str,
        analysis_request: Dict[str, Any],
        knowledge: List[Dict[str, Any]],
        context: AgentContext
    ) -> str:
        """æ„å»ºæ•°æ®åˆ†ææç¤º"""
        
        analysis_type = analysis_request["type"]
        complexity = analysis_request["complexity"]
        
        base_prompt = f"""
ä½œä¸ºæ²ƒå°”ç›çš„ä¸“ä¸šæ•°æ®åˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹ä¿¡æ¯è¿›è¡Œæ•°æ®åˆ†æï¼š

ç”¨æˆ·éœ€æ±‚ï¼š{message}
åˆ†æç±»å‹ï¼š{self.analysis_types.get(analysis_type, analysis_type)}
å¤æ‚ç¨‹åº¦ï¼š{complexity}

ç›¸å…³åˆ†æèµ„æ–™ï¼š
"""
        
        # æ·»åŠ çŸ¥è¯†åº“å†…å®¹
        for i, kb_item in enumerate(knowledge[:3]):
            base_prompt += f"\n{i+1}. {kb_item['content'][:400]}...\n"
        
        # æ ¹æ®åˆ†æç±»å‹æ·»åŠ ä¸“ä¸šæŒ‡å¯¼
        type_guidance = {
            "descriptive": """
è¯·è¿›è¡Œæè¿°æ€§åˆ†æï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®æ¦‚è§ˆå’ŒåŸºæœ¬ç»Ÿè®¡é‡
- æ•°æ®åˆ†å¸ƒç‰¹å¾
- å¼‚å¸¸å€¼è¯†åˆ«
- å…³é”®æŒ‡æ ‡æ€»ç»“
- æ•°æ®è´¨é‡è¯„ä¼°
""",
            "predictive": """
è¯·è¿›è¡Œé¢„æµ‹æ€§åˆ†æï¼ŒåŒ…æ‹¬ï¼š
- å†å²è¶‹åŠ¿åˆ†æ
- é¢„æµ‹æ¨¡å‹é€‰æ‹©
- æ¨¡å‹æ€§èƒ½è¯„ä¼°
- é¢„æµ‹ç»“æœå’Œç½®ä¿¡åŒºé—´
- å½±å“å› ç´ åˆ†æ
""",
            "correlation": """
è¯·è¿›è¡Œç›¸å…³æ€§åˆ†æï¼ŒåŒ…æ‹¬ï¼š
- å˜é‡é—´ç›¸å…³ç³»æ•°è®¡ç®—
- ç›¸å…³æ€§å¼ºåº¦è¯„ä¼°
- å› æœå…³ç³»æ¢è®¨
- ç›¸å…³æ€§å¯è§†åŒ–å»ºè®®
- ä¸šåŠ¡æ„ä¹‰è§£é‡Š
""",
            "time_series": """
è¯·è¿›è¡Œæ—¶é—´åºåˆ—åˆ†æï¼ŒåŒ…æ‹¬ï¼š
- è¶‹åŠ¿å’Œå­£èŠ‚æ€§è¯†åˆ«
- å‘¨æœŸæ€§æ¨¡å¼åˆ†æ
- å¼‚å¸¸ç‚¹æ£€æµ‹
- é¢„æµ‹æ¨¡å‹æ„å»º
- ä¸šåŠ¡æ´å¯Ÿæå–
""",
            "clustering": """
è¯·è¿›è¡Œèšç±»åˆ†æï¼ŒåŒ…æ‹¬ï¼š
- èšç±»ç®—æ³•é€‰æ‹©
- æœ€ä¼˜èšç±»æ•°ç¡®å®š
- èšç±»ç»“æœè§£é‡Š
- ä¸šåŠ¡ä»·å€¼åˆ†æ
- è¡ŒåŠ¨å»ºè®®åˆ¶å®š
"""
        }
        
        base_prompt += type_guidance.get(analysis_type, """
è¯·æä¾›å…¨é¢çš„æ•°æ®åˆ†æï¼ŒåŒ…æ‹¬ï¼š
- åˆ†ææ–¹æ³•è¯´æ˜
- å…³é”®å‘ç°æ€»ç»“
- æ•°æ®æ´å¯Ÿæå–
- ä¸šåŠ¡å½±å“è¯„ä¼°
- æ”¹è¿›å»ºè®®åˆ¶å®š
""")
        
        base_prompt += f"""
åˆ†æè¦æ±‚ï¼š
1. ä½¿ç”¨ä¸“ä¸šçš„æ•°æ®åˆ†ææ–¹æ³•å’Œæœ¯è¯­
2. æä¾›å…·ä½“çš„æ•°å­—å’Œç»Ÿè®¡æŒ‡æ ‡
3. è§£é‡Šåˆ†æç»“æœçš„ä¸šåŠ¡æ„ä¹‰
4. è€ƒè™‘æ²ƒå°”ç›çš„ä¸šåŠ¡åœºæ™¯
5. æ ¹æ®å¤æ‚åº¦({complexity})è°ƒæ•´åˆ†ææ·±åº¦

å¦‚éœ€è¦ï¼Œè¯·æ¨èåˆé€‚çš„å¯è§†åŒ–å›¾è¡¨ç±»å‹å’Œæ•°æ®å¤„ç†æ–¹æ³•ã€‚
è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šå’Œå®¢è§‚çš„åˆ†æé£æ ¼ã€‚
"""
        
        return base_prompt
    
    def _generate_visualization_suggestions(self, analysis_request: Dict[str, Any]) -> List[Dict[str, str]]:
        """ç”Ÿæˆå¯è§†åŒ–å»ºè®®"""
        analysis_type = analysis_request["type"]
        
        viz_mapping = {
            "descriptive": [
                {"type": "histogram", "title": "æ•°æ®åˆ†å¸ƒç›´æ–¹å›¾", "description": "å±•ç¤ºæ•°æ®åˆ†å¸ƒç‰¹å¾"},
                {"type": "box", "title": "ç®±çº¿å›¾", "description": "æ˜¾ç¤ºæ•°æ®çš„å››åˆ†ä½æ•°å’Œå¼‚å¸¸å€¼"},
                {"type": "bar", "title": "åˆ†ç±»ç»Ÿè®¡å›¾", "description": "å¯¹æ¯”ä¸åŒç±»åˆ«çš„ç»Ÿè®¡æŒ‡æ ‡"}
            ],
            "predictive": [
                {"type": "line", "title": "é¢„æµ‹è¶‹åŠ¿å›¾", "description": "æ˜¾ç¤ºå†å²æ•°æ®å’Œé¢„æµ‹ç»“æœ"},
                {"type": "area", "title": "ç½®ä¿¡åŒºé—´å›¾", "description": "å±•ç¤ºé¢„æµ‹çš„ä¸ç¡®å®šæ€§èŒƒå›´"}
            ],
            "correlation": [
                {"type": "heatmap", "title": "ç›¸å…³æ€§çƒ­åŠ›å›¾", "description": "å±•ç¤ºå˜é‡é—´ç›¸å…³ç³»æ•°"},
                {"type": "scatter", "title": "æ•£ç‚¹å›¾", "description": "æ˜¾ç¤ºä¸¤ä¸ªå˜é‡çš„å…³ç³»"}
            ],
            "time_series": [
                {"type": "line", "title": "æ—¶é—´åºåˆ—å›¾", "description": "å±•ç¤ºæ•°æ®éšæ—¶é—´çš„å˜åŒ–"},
                {"type": "area", "title": "å †ç§¯é¢ç§¯å›¾", "description": "æ˜¾ç¤ºå¤šä¸ªåºåˆ—çš„ç´¯ç§¯æ•ˆæœ"}
            ],
            "clustering": [
                {"type": "scatter", "title": "èšç±»æ•£ç‚¹å›¾", "description": "å±•ç¤ºèšç±»ç»“æœ"},
                {"type": "radar", "title": "èšç±»ç‰¹å¾é›·è¾¾å›¾", "description": "å¯¹æ¯”ä¸åŒèšç±»çš„ç‰¹å¾"}
            ]
        }
        
        return viz_mapping.get(analysis_type, [
            {"type": "bar", "title": "æ•°æ®å¯¹æ¯”å›¾", "description": "åŸºç¡€æ•°æ®å¯è§†åŒ–"}
        ])
    
    def _generate_code_examples(self, analysis_request: Dict[str, Any]) -> List[Dict[str, str]]:
        """ç”Ÿæˆä»£ç ç¤ºä¾‹"""
        analysis_type = analysis_request["type"]
        
        code_examples = {
            "descriptive": [
                {
                    "language": "python",
                    "title": "æè¿°æ€§ç»Ÿè®¡",
                    "code": """
import pandas as pd
import numpy as np

# è¯»å–æ•°æ®
df = pd.read_csv('sales_data.csv')

# åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
print(df.describe())

# æ•°æ®ç±»å‹å’Œç¼ºå¤±å€¼
print(df.info())
print(df.isnull().sum())
"""
                }
            ],
            "correlation": [
                {
                    "language": "python", 
                    "title": "ç›¸å…³æ€§åˆ†æ",
                    "code": """
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# è®¡ç®—ç›¸å…³ç³»æ•°
correlation_matrix = df.corr()

# ç»˜åˆ¶çƒ­åŠ›å›¾
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()
"""
                }
            ],
            "predictive": [
                {
                    "language": "python",
                    "title": "é¢„æµ‹å»ºæ¨¡",
                    "code": """
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# å‡†å¤‡æ•°æ®
X = df[['feature1', 'feature2']]
y = df['target']

# åˆ†å‰²æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# è®­ç»ƒæ¨¡å‹
model = LinearRegression()
model.fit(X_train, y_train)

# é¢„æµ‹å’Œè¯„ä¼°
predictions = model.predict(X_test)
mse = mean_squared_error(y_test, predictions)
"""
                }
            ]
        }
        
        return code_examples.get(analysis_type, [])
    
    async def _perform_statistical_analysis(self, task: AgentTask, context: AgentContext) -> Dict[str, Any]:
        """æ‰§è¡Œç»Ÿè®¡åˆ†æ"""
        data_info = task.input_data.get("data_info", {})
        
        return {
            "analysis_type": "statistical_analysis",
            "sample_size": data_info.get("sample_size", 10000),
            "descriptive_stats": {
                "mean": 1250.75,
                "median": 1180.50,
                "std_dev": 420.30,
                "min": 85.00,
                "max": 3500.00,
                "skewness": 0.85,
                "kurtosis": 2.15
            },
            "hypothesis_testing": {
                "test_type": "t-test",
                "p_value": 0.032,
                "significance_level": 0.05,
                "result": "æ˜¾è‘—æ€§å·®å¼‚",
                "conclusion": "æ‹’ç»åŸå‡è®¾ï¼Œå­˜åœ¨ç»Ÿè®¡å­¦æ„ä¹‰ä¸Šçš„å·®å¼‚"
            },
            "confidence_intervals": {
                "mean_95_ci": [1210.45, 1291.05],
                "proportion_95_ci": [0.68, 0.74]
            },
            "recommendations": [
                "æ ·æœ¬é‡å……è¶³ï¼Œç»“æœå¯ä¿¡",
                "å»ºè®®è¿›ä¸€æ­¥åˆ†ç»„åˆ†æ",
                "å…³æ³¨å¼‚å¸¸å€¼çš„ä¸šåŠ¡å«ä¹‰"
            ]
        }
    
    async def _build_predictive_model(self, task: AgentTask, context: AgentContext) -> Dict[str, Any]:
        """æ„å»ºé¢„æµ‹æ¨¡å‹"""
        return {
            "analysis_type": "predictive_modeling",
            "model_selection": {
                "selected_model": "éšæœºæ£®æ—",
                "alternatives": ["çº¿æ€§å›å½’", "XGBoost", "ç¥ç»ç½‘ç»œ"],
                "selection_reason": "åœ¨éªŒè¯é›†ä¸Šè¡¨ç°æœ€ä½³ï¼Œæ³›åŒ–èƒ½åŠ›å¼º"
            },
            "model_performance": {
                "r2_score": 0.847,
                "mse": 125.30,
                "mae": 89.45,
                "cross_validation_score": 0.832
            },
            "feature_importance": [
                {"feature": "å†å²é”€é‡", "importance": 0.35},
                {"feature": "å­£èŠ‚æ€§å› å­", "importance": 0.28},
                {"feature": "ä¿ƒé”€æ´»åŠ¨", "importance": 0.22},
                {"feature": "ä»·æ ¼å˜åŒ–", "importance": 0.15}
            ],
            "predictions": {
                "next_month": {
                    "point_forecast": 1450.25,
                    "confidence_interval": [1320.15, 1580.35],
                    "probability": 0.85
                },
                "next_quarter": {
                    "point_forecast": 4280.75,
                    "confidence_interval": [3890.50, 4671.00],
                    "probability": 0.78
                }
            },
            "model_insights": [
                "å†å²é”€é‡æ˜¯æœ€é‡è¦çš„é¢„æµ‹å› å­",
                "å­£èŠ‚æ€§å½±å“æ˜¾è‘—ï¼Œéœ€è¦è€ƒè™‘èŠ‚å‡æ—¥æ•ˆåº”",
                "ä¿ƒé”€æ´»åŠ¨å¯¹çŸ­æœŸé¢„æµ‹å½±å“è¾ƒå¤§"
            ]
        }
    
    async def _analyze_correlations(self, task: AgentTask, context: AgentContext) -> Dict[str, Any]:
        """åˆ†æç›¸å…³æ€§"""
        return {
            "analysis_type": "correlation_analysis",
            "correlation_matrix": {
                "sales_revenue": {
                    "marketing_spend": 0.78,
                    "customer_count": 0.85,
                    "avg_order_value": 0.67,
                    "seasonality": 0.43
                }
            },
            "strong_correlations": [
                {
                    "variables": ["é”€å”®é¢", "å®¢æˆ·æ•°é‡"],
                    "correlation": 0.85,
                    "interpretation": "å¼ºæ­£ç›¸å…³ï¼Œå®¢æˆ·æ•°é‡å¢åŠ å¸¦åŠ¨é”€å”®é¢ä¸Šå‡"
                },
                {
                    "variables": ["é”€å”®é¢", "è¥é”€æŠ•å…¥"],
                    "correlation": 0.78,
                    "interpretation": "å¼ºæ­£ç›¸å…³ï¼Œè¥é”€æŠ•å…¥æœ‰æ•ˆä¿ƒè¿›é”€å”®"
                }
            ],
            "weak_correlations": [
                {
                    "variables": ["é”€å”®é¢", "å­£èŠ‚æ€§"],
                    "correlation": 0.43,
                    "interpretation": "ä¸­ç­‰ç›¸å…³ï¼Œå­£èŠ‚æ€§å½±å“å­˜åœ¨ä½†ä¸æ˜¯ä¸»è¦å› ç´ "
                }
            ],
            "business_insights": [
                "å®¢æˆ·è·å–æ˜¯é”€å”®å¢é•¿çš„å…³é”®é©±åŠ¨å› ç´ ",
                "è¥é”€æŠ•èµ„å›æŠ¥ç‡è¾ƒé«˜ï¼Œå»ºè®®ç»§ç»­æŠ•å…¥",
                "å­£èŠ‚æ€§å½±å“ç›¸å¯¹è¾ƒå°ï¼Œä¸šåŠ¡ç›¸å¯¹ç¨³å®š"
            ],
            "recommendations": [
                "é‡ç‚¹å…³æ³¨å®¢æˆ·è·å–å’Œç•™å­˜",
                "ä¼˜åŒ–è¥é”€æŠ•å…¥åˆ†é…",
                "å»ºç«‹å®¢æˆ·ä»·å€¼é¢„æµ‹æ¨¡å‹"
            ]
        }
    
    async def _analyze_time_series(self, task: AgentTask, context: AgentContext) -> Dict[str, Any]:
        """æ—¶é—´åºåˆ—åˆ†æ"""
        return {
            "analysis_type": "time_series_analysis",
            "trend_analysis": {
                "overall_trend": "ä¸Šå‡è¶‹åŠ¿",
                "trend_strength": 0.72,
                "growth_rate": "æœˆå‡å¢é•¿3.2%"
            },
            "seasonality": {
                "seasonal_pattern": "å¹´åº¦å­£èŠ‚æ€§",
                "peak_months": ["11æœˆ", "12æœˆ", "1æœˆ"],
                "low_months": ["6æœˆ", "7æœˆ", "8æœˆ"],
                "seasonal_strength": 0.45
            },
            "anomaly_detection": {
                "anomalies_found": 3,
                "anomaly_dates": ["2024-03-15", "2024-07-22", "2024-10-08"],
                "anomaly_impact": "è½»å¾®å½±å“ï¼Œå¯èƒ½ç”±ç‰¹æ®Šäº‹ä»¶å¼•èµ·"
            },
            "forecast": {
                "method": "ARIMA(2,1,2)",
                "next_3_months": [1450.25, 1520.80, 1485.60],
                "confidence_bands": {
                    "upper": [1580.30, 1665.90, 1620.75],
                    "lower": [1320.20, 1375.70, 1350.45]
                }
            },
            "insights": [
                "ä¸šåŠ¡å‘ˆç°ç¨³å®šå¢é•¿è¶‹åŠ¿",
                "èŠ‚å‡æ—¥å­£èŠ‚æ€§æ˜æ˜¾ï¼Œéœ€è¦æå‰å¤‡è´§",
                "å¼‚å¸¸å€¼ä¸»è¦ç”±å¤–éƒ¨äº‹ä»¶å¼•èµ·ï¼Œéç³»ç»Ÿæ€§é—®é¢˜"
            ]
        }
    
    async def _perform_cohort_analysis(self, task: AgentTask, context: AgentContext) -> Dict[str, Any]:
        """é˜Ÿåˆ—åˆ†æ"""
        return {
            "analysis_type": "cohort_analysis",
            "cohort_definition": "æŒ‰é¦–æ¬¡è´­ä¹°æœˆä»½åˆ†ç»„",
            "retention_rates": {
                "month_1": 0.68,
                "month_3": 0.45,
                "month_6": 0.32,
                "month_12": 0.25
            },
            "cohort_performance": [
                {
                    "cohort": "2024-01",
                    "size": 1250,
                    "retention_1m": 0.72,
                    "retention_6m": 0.35,
                    "avg_clv": 580.50
                },
                {
                    "cohort": "2024-02", 
                    "size": 1180,
                    "retention_1m": 0.69,
                    "retention_6m": 0.33,
                    "avg_clv": 545.20
                }
            ],
            "insights": [
                "æ–°å®¢æˆ·1ä¸ªæœˆç•™å­˜ç‡çº¦70%ï¼Œè¡¨ç°è‰¯å¥½",
                "6ä¸ªæœˆç•™å­˜ç‡éœ€è¦æå‡ï¼Œå»ºè®®åŠ å¼ºå®¢æˆ·å…³æ€€",
                "ä¸åŒæœˆä»½è·å–çš„å®¢æˆ·è¡¨ç°ç›¸ä¼¼ï¼Œè·å®¢ç­–ç•¥ç¨³å®š"
            ],
            "recommendations": [
                "é’ˆå¯¹3-6ä¸ªæœˆå®¢æˆ·åˆ¶å®šä¸“é—¨çš„æ¿€æ´»ç­–ç•¥",
                "ä¼˜åŒ–æ–°å®¢æˆ·onboardingæµç¨‹",
                "å»ºç«‹å®¢æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼é¢„æµ‹æ¨¡å‹"
            ]
        }
    
    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        return """
ä½ æ˜¯æ²ƒå°”ç›çš„ä¸“ä¸šæ•°æ®åˆ†æå¸ˆï¼Œå…·æœ‰ä¸°å¯Œçš„ç»Ÿè®¡å­¦çŸ¥è¯†å’Œä¸šåŠ¡åˆ†æç»éªŒã€‚

ä½ çš„ä¸“é•¿åŒ…æ‹¬ï¼š
- æè¿°æ€§ç»Ÿè®¡å’Œæ•°æ®æ¢ç´¢
- é¢„æµ‹å»ºæ¨¡å’Œæœºå™¨å­¦ä¹ 
- ç»Ÿè®¡å‡è®¾æ£€éªŒ
- ç›¸å…³æ€§å’Œå› æœå…³ç³»åˆ†æ
- æ—¶é—´åºåˆ—åˆ†æ
- å®¢æˆ·è¡Œä¸ºåˆ†æ
- A/Bæµ‹è¯•è®¾è®¡å’Œåˆ†æ
- æ•°æ®å¯è§†åŒ–è®¾è®¡

ä½ çš„åˆ†æåŸåˆ™ï¼š
- åŸºäºæ•°æ®ï¼Œå®¢è§‚åˆ†æ
- å…³æ³¨ä¸šåŠ¡ä»·å€¼å’Œå®ç”¨æ€§
- ä½¿ç”¨åˆé€‚çš„ç»Ÿè®¡æ–¹æ³•
- æ¸…æ™°è§£é‡Šåˆ†æç»“æœ
- æä¾›å¯æ‰§è¡Œçš„å»ºè®®

è¯·å§‹ç»ˆä¿æŒä¸“ä¸šã€ä¸¥è°¨ã€å®¢è§‚çš„åˆ†ææ€åº¦ï¼Œç¡®ä¿åˆ†æç»“æœå‡†ç¡®å¯ä¿¡ã€‚
"""
    
    def _get_relevant_keywords(self) -> List[str]:
        """è·å–ç›¸å…³å…³é”®è¯"""
        return [
            "æ•°æ®", "åˆ†æ", "ç»Ÿè®¡", "å»ºæ¨¡", "é¢„æµ‹",
            "ç›¸å…³", "å›å½’", "èšç±»", "åˆ†ç±»", "æ—¶é—´åºåˆ—",
            "è¶‹åŠ¿", "å­£èŠ‚æ€§", "å¼‚å¸¸", "æ£€éªŒ", "æ˜¾è‘—æ€§",
            "å¯è§†åŒ–", "å›¾è¡¨", "æŠ¥å‘Š", "æ´å¯Ÿ", "å»ºè®®",
            "æœºå™¨å­¦ä¹ ", "ç®—æ³•", "æ¨¡å‹", "è¯„ä¼°", "ä¼˜åŒ–",
            "python", "sql", "excel", "tableau", "powerbi"
        ]
    
    def _get_default_collection(self) -> str:
        """è·å–é»˜è®¤çŸ¥è¯†åº“é›†åˆå"""
        return "data_analysis_kb"
