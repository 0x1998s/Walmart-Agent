# ğŸ›’ æ²ƒå°”ç›AI Agentå¹³å° - å¤§è¯­è¨€æ¨¡å‹æœåŠ¡
# Walmart AI Agent Platform - Large Language Model Service

import asyncio
import json
import logging
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, AsyncGenerator
from enum import Enum

import httpx
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic

from app.core.config import get_settings

settings = get_settings()
logger = logging.getLogger(__name__)


class ModelProvider(str, Enum):
    """æ¨¡å‹æä¾›å•†æšä¸¾"""
    OPENAI = "openai"
    CHATGLM = "chatglm"
    DEEPSEEK = "deepseek"
    ANTHROPIC = "anthropic"


class LLMResponse:
    """LLMå“åº”åŒ…è£…ç±»"""
    
    def __init__(
        self,
        content: str,
        model: str,
        provider: ModelProvider,
        tokens_used: Optional[int] = None,
        cost: Optional[float] = None,
        metadata: Optional[Dict[str, Any]] = None
    ):
        self.content = content
        self.model = model
        self.provider = provider
        self.tokens_used = tokens_used
        self.cost = cost
        self.metadata = metadata or {}
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "content": self.content,
            "model": self.model,
            "provider": self.provider,
            "tokens_used": self.tokens_used,
            "cost": self.cost,
            "metadata": self.metadata
        }


class BaseLLMProvider(ABC):
    """LLMæä¾›å•†åŸºç±»"""
    
    def __init__(self, api_key: str, base_url: str, model: str):
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.client = None
    
    @abstractmethod
    async def initialize(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        pass
    
    @abstractmethod
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        **kwargs
    ) -> LLMResponse:
        """èŠå¤©å®Œæˆ"""
        pass
    
    @abstractmethod
    async def stream_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """æµå¼èŠå¤©å®Œæˆ"""
        pass


class OpenAIProvider(BaseLLMProvider):
    """OpenAIæ¨¡å‹æä¾›å•†"""
    
    def __init__(self):
        super().__init__(
            api_key=settings.OPENAI_API_KEY,
            base_url=settings.OPENAI_BASE_URL,
            model=settings.OPENAI_MODEL
        )
    
    async def initialize(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        if not self.api_key:
            raise ValueError("OpenAI APIå¯†é’¥æœªè®¾ç½®")
        
        self.client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        logger.info("âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        **kwargs
    ) -> LLMResponse:
        """OpenAIèŠå¤©å®Œæˆ"""
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=stream,
                **kwargs
            )
            
            content = response.choices[0].message.content
            tokens_used = response.usage.total_tokens if response.usage else None
            
            return LLMResponse(
                content=content,
                model=self.model,
                provider=ModelProvider.OPENAI,
                tokens_used=tokens_used,
                metadata={
                    "finish_reason": response.choices[0].finish_reason,
                    "response_id": response.id
                }
            )
            
        except Exception as e:
            logger.error(f"âŒ OpenAIèŠå¤©å®Œæˆå¤±è´¥: {e}")
            raise
    
    async def stream_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """OpenAIæµå¼èŠå¤©å®Œæˆ"""
        try:
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True,
                **kwargs
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"âŒ OpenAIæµå¼èŠå¤©å¤±è´¥: {e}")
            raise


class ChatGLMProvider(BaseLLMProvider):
    """ChatGLMæ¨¡å‹æä¾›å•†"""
    
    def __init__(self):
        super().__init__(
            api_key=settings.CHATGLM_API_KEY,
            base_url=settings.CHATGLM_BASE_URL,
            model=settings.CHATGLM_MODEL
        )
    
    async def initialize(self):
        """åˆå§‹åŒ–ChatGLMå®¢æˆ·ç«¯"""
        if not self.api_key:
            raise ValueError("ChatGLM APIå¯†é’¥æœªè®¾ç½®")
        
        self.client = httpx.AsyncClient(
            base_url=self.base_url,
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            timeout=60.0
        )
        logger.info("âœ… ChatGLMå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        **kwargs
    ) -> LLMResponse:
        """ChatGLMèŠå¤©å®Œæˆ"""
        try:
            payload = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "stream": stream,
                **kwargs
            }
            
            if max_tokens:
                payload["max_tokens"] = max_tokens
            
            response = await self.client.post("/chat/completions", json=payload)
            response.raise_for_status()
            
            data = response.json()
            content = data["choices"][0]["message"]["content"]
            tokens_used = data.get("usage", {}).get("total_tokens")
            
            return LLMResponse(
                content=content,
                model=self.model,
                provider=ModelProvider.CHATGLM,
                tokens_used=tokens_used,
                metadata={
                    "finish_reason": data["choices"][0].get("finish_reason"),
                    "response_id": data.get("id")
                }
            )
            
        except Exception as e:
            logger.error(f"âŒ ChatGLMèŠå¤©å®Œæˆå¤±è´¥: {e}")
            raise
    
    async def stream_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """ChatGLMæµå¼èŠå¤©å®Œæˆ"""
        try:
            payload = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "stream": True,
                **kwargs
            }
            
            if max_tokens:
                payload["max_tokens"] = max_tokens
            
            async with self.client.stream("POST", "/chat/completions", json=payload) as response:
                response.raise_for_status()
                
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data_str = line[6:]  # å»æ‰ "data: " å‰ç¼€
                        
                        if data_str.strip() == "[DONE]":
                            break
                        
                        try:
                            data = json.loads(data_str)
                            content = data["choices"][0]["delta"].get("content", "")
                            if content:
                                yield content
                        except json.JSONDecodeError:
                            continue
                            
        except Exception as e:
            logger.error(f"âŒ ChatGLMæµå¼èŠå¤©å¤±è´¥: {e}")
            raise


class DeepSeekProvider(BaseLLMProvider):
    """DeepSeekæ¨¡å‹æä¾›å•†"""
    
    def __init__(self):
        super().__init__(
            api_key=settings.DEEPSEEK_API_KEY,
            base_url=settings.DEEPSEEK_BASE_URL,
            model=settings.DEEPSEEK_MODEL
        )
    
    async def initialize(self):
        """åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯"""
        if not self.api_key:
            raise ValueError("DeepSeek APIå¯†é’¥æœªè®¾ç½®")
        
        self.client = httpx.AsyncClient(
            base_url=self.base_url,
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            timeout=60.0
        )
        logger.info("âœ… DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        **kwargs
    ) -> LLMResponse:
        """DeepSeekèŠå¤©å®Œæˆ"""
        try:
            payload = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "stream": stream,
                **kwargs
            }
            
            if max_tokens:
                payload["max_tokens"] = max_tokens
            
            response = await self.client.post("/chat/completions", json=payload)
            response.raise_for_status()
            
            data = response.json()
            content = data["choices"][0]["message"]["content"]
            tokens_used = data.get("usage", {}).get("total_tokens")
            
            return LLMResponse(
                content=content,
                model=self.model,
                provider=ModelProvider.DEEPSEEK,
                tokens_used=tokens_used,
                metadata={
                    "finish_reason": data["choices"][0].get("finish_reason"),
                    "response_id": data.get("id")
                }
            )
            
        except Exception as e:
            logger.error(f"âŒ DeepSeekèŠå¤©å®Œæˆå¤±è´¥: {e}")
            raise
    
    async def stream_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """DeepSeekæµå¼èŠå¤©å®Œæˆ"""
        try:
            payload = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "stream": True,
                **kwargs
            }
            
            if max_tokens:
                payload["max_tokens"] = max_tokens
            
            async with self.client.stream("POST", "/chat/completions", json=payload) as response:
                response.raise_for_status()
                
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data_str = line[6:]
                        
                        if data_str.strip() == "[DONE]":
                            break
                        
                        try:
                            data = json.loads(data_str)
                            content = data["choices"][0]["delta"].get("content", "")
                            if content:
                                yield content
                        except json.JSONDecodeError:
                            continue
                            
        except Exception as e:
            logger.error(f"âŒ DeepSeekæµå¼èŠå¤©å¤±è´¥: {e}")
            raise


class LLMService:
    """å¤§è¯­è¨€æ¨¡å‹æœåŠ¡"""
    
    def __init__(self):
        self.providers: Dict[ModelProvider, BaseLLMProvider] = {}
        self.default_provider = ModelProvider.OPENAI
    
    async def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„LLMæä¾›å•†"""
        # åˆå§‹åŒ–OpenAI
        if settings.OPENAI_API_KEY:
            openai_provider = OpenAIProvider()
            await openai_provider.initialize()
            self.providers[ModelProvider.OPENAI] = openai_provider
        
        # åˆå§‹åŒ–ChatGLM
        if settings.CHATGLM_API_KEY:
            chatglm_provider = ChatGLMProvider()
            await chatglm_provider.initialize()
            self.providers[ModelProvider.CHATGLM] = chatglm_provider
        
        # åˆå§‹åŒ–DeepSeek
        if settings.DEEPSEEK_API_KEY:
            deepseek_provider = DeepSeekProvider()
            await deepseek_provider.initialize()
            self.providers[ModelProvider.DEEPSEEK] = deepseek_provider
        
        # è®¾ç½®é»˜è®¤æä¾›å•†
        if self.providers:
            self.default_provider = next(iter(self.providers.keys()))
        
        logger.info(f"âœ… LLMæœåŠ¡åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨æä¾›å•†: {list(self.providers.keys())}")
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        provider: Optional[ModelProvider] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        **kwargs
    ) -> LLMResponse:
        """èŠå¤©å®Œæˆ"""
        provider = provider or self.default_provider
        
        if provider not in self.providers:
            raise ValueError(f"æä¾›å•† {provider} ä¸å¯ç”¨")
        
        return await self.providers[provider].chat_completion(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=stream,
            **kwargs
        )
    
    async def stream_completion(
        self,
        messages: List[Dict[str, str]],
        provider: Optional[ModelProvider] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """æµå¼èŠå¤©å®Œæˆ"""
        provider = provider or self.default_provider
        
        if provider not in self.providers:
            raise ValueError(f"æä¾›å•† {provider} ä¸å¯ç”¨")
        
        async for chunk in self.providers[provider].stream_completion(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        ):
            yield chunk
    
    async def multi_provider_completion(
        self,
        messages: List[Dict[str, str]],
        providers: List[ModelProvider],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> List[LLMResponse]:
        """å¤šæä¾›å•†å¹¶è¡Œå®Œæˆ"""
        tasks = []
        
        for provider in providers:
            if provider in self.providers:
                task = self.chat_completion(
                    messages=messages,
                    provider=provider,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    **kwargs
                )
                tasks.append(task)
        
        if not tasks:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æä¾›å•†")
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤å¼‚å¸¸ç»“æœ
        valid_results = [r for r in results if isinstance(r, LLMResponse)]
        
        return valid_results
    
    async def get_best_response(
        self,
        messages: List[Dict[str, str]],
        providers: Optional[List[ModelProvider]] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> LLMResponse:
        """è·å–æœ€ä½³å“åº”ï¼ˆåŸºäºå¤šæä¾›å•†æ¯”è¾ƒï¼‰"""
        providers = providers or list(self.providers.keys())
        
        responses = await self.multi_provider_completion(
            messages=messages,
            providers=providers,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
        
        if not responses:
            raise RuntimeError("æ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥äº†")
        
        # ç®€å•çš„è¯„åˆ†æœºåˆ¶ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
        def score_response(response: LLMResponse) -> float:
            score = len(response.content)  # åŸºç¡€åˆ†æ•°ï¼šå†…å®¹é•¿åº¦
            
            # æä¾›å•†æƒé‡
            provider_weights = {
                ModelProvider.OPENAI: 1.2,
                ModelProvider.CHATGLM: 1.1,
                ModelProvider.DEEPSEEK: 1.0,
            }
            score *= provider_weights.get(response.provider, 1.0)
            
            return score
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å“åº”
        best_response = max(responses, key=score_response)
        
        return best_response
    
    def get_available_providers(self) -> List[ModelProvider]:
        """è·å–å¯ç”¨çš„æä¾›å•†åˆ—è¡¨"""
        return list(self.providers.keys())
    
    async def health_check(self) -> Dict[str, bool]:
        """å¥åº·æ£€æŸ¥æ‰€æœ‰æä¾›å•†"""
        health_status = {}
        
        test_messages = [{"role": "user", "content": "Hello"}]
        
        for provider, client in self.providers.items():
            try:
                await client.chat_completion(test_messages, max_tokens=10)
                health_status[provider] = True
            except Exception as e:
                logger.warning(f"æä¾›å•† {provider} å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
                health_status[provider] = False
        
        return health_status
