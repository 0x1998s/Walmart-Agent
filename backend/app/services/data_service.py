# ğŸ›’ æ²ƒå°”ç›AI Agentå¹³å° - æ•°æ®å¤„ç†æœåŠ¡
# Walmart AI Agent Platform - Data Processing Service

import asyncio
import json
import logging
import mimetypes
from io import BytesIO
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union
from uuid import uuid4

import pandas as pd
import numpy as np
from docx import Document as DocxDocument
from openpyxl import load_workbook
import PyPDF2
from pptx import Presentation

from app.core.config import get_settings
from app.services.vector_service import VectorService

settings = get_settings()
logger = logging.getLogger(__name__)


class DataService:
    """æ•°æ®å¤„ç†æœåŠ¡ - å¤„ç†ç»“æ„åŒ–å’Œéç»“æ„åŒ–æ•°æ®"""
    
    def __init__(self):
        self.vector_service: Optional[VectorService] = None
        self.supported_formats = {
            # æ–‡æ¡£æ ¼å¼
            '.pdf': self._process_pdf,
            '.docx': self._process_docx,
            '.doc': self._process_docx,  # ç®€åŒ–å¤„ç†
            '.txt': self._process_text,
            '.md': self._process_text,
            
            # è¡¨æ ¼æ ¼å¼
            '.xlsx': self._process_excel,
            '.xls': self._process_excel,
            '.csv': self._process_csv,
            
            # æ¼”ç¤ºæ–‡ç¨¿
            '.pptx': self._process_pptx,
            '.ppt': self._process_pptx,  # ç®€åŒ–å¤„ç†
            
            # æ•°æ®æ ¼å¼
            '.json': self._process_json,
            '.xml': self._process_xml,
        }
    
    async def initialize(self, vector_service: VectorService):
        """åˆå§‹åŒ–æ•°æ®æœåŠ¡"""
        self.vector_service = vector_service
        logger.info("âœ… æ•°æ®å¤„ç†æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    async def process_file(
        self,
        file_path: Union[str, Path],
        file_content: Optional[bytes] = None,
        metadata: Optional[Dict[str, Any]] = None,
        collection_name: str = "walmart_documents"
    ) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            file_path = Path(file_path)
            file_extension = file_path.suffix.lower()
            
            if file_extension not in self.supported_formats:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}")
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            if file_content is None:
                with open(file_path, 'rb') as f:
                    file_content = f.read()
            
            # å¤„ç†æ–‡ä»¶
            processor = self.supported_formats[file_extension]
            processed_data = await processor(file_content, file_path)
            
            # å‡†å¤‡å…ƒæ•°æ®
            file_metadata = {
                "filename": file_path.name,
                "file_type": file_extension,
                "file_size": len(file_content),
                "processed_at": pd.Timestamp.now().isoformat(),
                **(metadata or {})
            }
            
            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
            if self.vector_service and processed_data.get("text_chunks"):
                doc_ids = await self.vector_service.add_documents(
                    collection_name=collection_name,
                    documents=processed_data["text_chunks"],
                    metadatas=[
                        {**file_metadata, "chunk_index": i}
                        for i in range(len(processed_data["text_chunks"]))
                    ]
                )
                processed_data["document_ids"] = doc_ids
            
            logger.info(f"âœ… æˆåŠŸå¤„ç†æ–‡ä»¶: {file_path.name}")
            return {
                "status": "success",
                "file_path": str(file_path),
                "metadata": file_metadata,
                **processed_data
            }
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return {
                "status": "error",
                "file_path": str(file_path),
                "error": str(e)
            }
    
    async def process_batch_files(
        self,
        file_paths: List[Union[str, Path]],
        collection_name: str = "walmart_documents",
        batch_size: int = 10
    ) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†æ–‡ä»¶"""
        results = []
        
        for i in range(0, len(file_paths), batch_size):
            batch = file_paths[i:i + batch_size]
            
            # å¹¶å‘å¤„ç†æ‰¹æ¬¡
            batch_tasks = [
                self.process_file(file_path, collection_name=collection_name)
                for file_path in batch
            ]
            
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            for result in batch_results:
                if isinstance(result, Exception):
                    results.append({
                        "status": "error",
                        "error": str(result)
                    })
                else:
                    results.append(result)
        
        logger.info(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶")
        return results
    
    async def _process_pdf(self, content: bytes, file_path: Path) -> Dict[str, Any]:
        """å¤„ç†PDFæ–‡ä»¶"""
        try:
            pdf_reader = PyPDF2.PdfReader(BytesIO(content))
            
            text_content = ""
            page_texts = []
            
            for page_num, page in enumerate(pdf_reader.pages):
                page_text = page.extract_text()
                text_content += page_text + "\n"
                page_texts.append({
                    "page": page_num + 1,
                    "text": page_text.strip()
                })
            
            # åˆ†å—å¤„ç†
            text_chunks = self._chunk_text(text_content)
            
            return {
                "type": "pdf",
                "pages": len(pdf_reader.pages),
                "text_content": text_content.strip(),
                "text_chunks": text_chunks,
                "page_texts": page_texts,
                "metadata": {
                    "page_count": len(pdf_reader.pages),
                    "has_metadata": bool(pdf_reader.metadata),
                }
            }
            
        except Exception as e:
            raise RuntimeError(f"PDFå¤„ç†å¤±è´¥: {e}")
    
    async def _process_docx(self, content: bytes, file_path: Path) -> Dict[str, Any]:
        """å¤„ç†Wordæ–‡æ¡£"""
        try:
            doc = DocxDocument(BytesIO(content))
            
            paragraphs = []
            text_content = ""
            
            for para in doc.paragraphs:
                if para.text.strip():
                    paragraphs.append(para.text.strip())
                    text_content += para.text + "\n"
            
            # å¤„ç†è¡¨æ ¼
            tables_data = []
            for table in doc.tables:
                table_data = []
                for row in table.rows:
                    row_data = [cell.text.strip() for cell in row.cells]
                    table_data.append(row_data)
                tables_data.append(table_data)
            
            text_chunks = self._chunk_text(text_content)
            
            return {
                "type": "docx",
                "text_content": text_content.strip(),
                "text_chunks": text_chunks,
                "paragraphs": paragraphs,
                "tables": tables_data,
                "metadata": {
                    "paragraph_count": len(paragraphs),
                    "table_count": len(tables_data),
                }
            }
            
        except Exception as e:
            raise RuntimeError(f"Wordæ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
    
    async def _process_text(self, content: bytes, file_path: Path) -> Dict[str, Any]:
        """å¤„ç†æ–‡æœ¬æ–‡ä»¶"""
        try:
            # å°è¯•ä¸åŒç¼–ç 
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            text_content = None
            
            for encoding in encodings:
                try:
                    text_content = content.decode(encoding)
                    break
                except UnicodeDecodeError:
                    continue
            
            if text_content is None:
                raise ValueError("æ— æ³•è§£ç æ–‡æœ¬æ–‡ä»¶")
            
            lines = text_content.splitlines()
            text_chunks = self._chunk_text(text_content)
            
            return {
                "type": "text",
                "text_content": text_content,
                "text_chunks": text_chunks,
                "lines": lines,
                "metadata": {
                    "line_count": len(lines),
                    "character_count": len(text_content),
                }
            }
            
        except Exception as e:
            raise RuntimeError(f"æ–‡æœ¬æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
    
    async def _process_excel(self, content: bytes, file_path: Path) -> Dict[str, Any]:
        """å¤„ç†Excelæ–‡ä»¶"""
        try:
            # ä½¿ç”¨pandasè¯»å–Excel
            excel_data = pd.read_excel(BytesIO(content), sheet_name=None)
            
            sheets_data = {}
            text_content = ""
            
            for sheet_name, df in excel_data.items():
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                sheet_dict = df.to_dict('records')
                sheets_data[sheet_name] = sheet_dict
                
                # æå–æ–‡æœ¬å†…å®¹
                sheet_text = f"å·¥ä½œè¡¨: {sheet_name}\n"
                sheet_text += df.to_string(index=False)
                text_content += sheet_text + "\n\n"
            
            text_chunks = self._chunk_text(text_content)
            
            return {
                "type": "excel",
                "text_content": text_content.strip(),
                "text_chunks": text_chunks,
                "sheets": sheets_data,
                "metadata": {
                    "sheet_count": len(excel_data),
                    "sheet_names": list(excel_data.keys()),
                }
            }
            
        except Exception as e:
            raise RuntimeError(f"Excelæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
    
    async def _process_csv(self, content: bytes, file_path: Path) -> Dict[str, Any]:
        """å¤„ç†CSVæ–‡ä»¶"""
        try:
            # å°è¯•ä¸åŒç¼–ç 
            encodings = ['utf-8', 'gbk', 'gb2312']
            df = None
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(BytesIO(content), encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue
            
            if df is None:
                raise ValueError("æ— æ³•è§£ç CSVæ–‡ä»¶")
            
            # è½¬æ¢ä¸ºæ–‡æœ¬
            text_content = df.to_string(index=False)
            text_chunks = self._chunk_text(text_content)
            
            # æ•°æ®ç»Ÿè®¡
            stats = {
                "row_count": len(df),
                "column_count": len(df.columns),
                "columns": df.columns.tolist(),
                "dtypes": df.dtypes.to_dict(),
            }
            
            return {
                "type": "csv",
                "text_content": text_content,
                "text_chunks": text_chunks,
                "data": df.to_dict('records'),
                "metadata": stats
            }
            
        except Exception as e:
            raise RuntimeError(f"CSVæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
    
    async def _process_pptx(self, content: bytes, file_path: Path) -> Dict[str, Any]:
        """å¤„ç†PowerPointæ–‡ä»¶"""
        try:
            prs = Presentation(BytesIO(content))
            
            slides_data = []
            text_content = ""
            
            for slide_num, slide in enumerate(prs.slides):
                slide_text = f"å¹»ç¯ç‰‡ {slide_num + 1}:\n"
                
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text:
                        slide_text += shape.text + "\n"
                
                slides_data.append({
                    "slide": slide_num + 1,
                    "text": slide_text.strip()
                })
                text_content += slide_text + "\n"
            
            text_chunks = self._chunk_text(text_content)
            
            return {
                "type": "pptx",
                "text_content": text_content.strip(),
                "text_chunks": text_chunks,
                "slides": slides_data,
                "metadata": {
                    "slide_count": len(prs.slides),
                }
            }
            
        except Exception as e:
            raise RuntimeError(f"PowerPointæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
    
    async def _process_json(self, content: bytes, file_path: Path) -> Dict[str, Any]:
        """å¤„ç†JSONæ–‡ä»¶"""
        try:
            json_data = json.loads(content.decode('utf-8'))
            
            # è½¬æ¢ä¸ºæ–‡æœ¬è¡¨ç¤º
            text_content = json.dumps(json_data, ensure_ascii=False, indent=2)
            text_chunks = self._chunk_text(text_content)
            
            return {
                "type": "json",
                "text_content": text_content,
                "text_chunks": text_chunks,
                "data": json_data,
                "metadata": {
                    "json_structure": self._analyze_json_structure(json_data),
                }
            }
            
        except Exception as e:
            raise RuntimeError(f"JSONæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
    
    async def _process_xml(self, content: bytes, file_path: Path) -> Dict[str, Any]:
        """å¤„ç†XMLæ–‡ä»¶"""
        try:
            import xml.etree.ElementTree as ET
            
            root = ET.fromstring(content.decode('utf-8'))
            
            # æå–æ–‡æœ¬å†…å®¹
            text_content = self._extract_xml_text(root)
            text_chunks = self._chunk_text(text_content)
            
            return {
                "type": "xml",
                "text_content": text_content,
                "text_chunks": text_chunks,
                "metadata": {
                    "root_tag": root.tag,
                    "element_count": len(list(root.iter())),
                }
            }
            
        except Exception as e:
            raise RuntimeError(f"XMLæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
    
    def _chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å—"""
        if not text:
            return []
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€å—ï¼Œå°è¯•åœ¨å¥å·å¤„åˆ†å‰²
            if end < len(text):
                last_period = text.rfind('.', start, end)
                last_newline = text.rfind('\n', start, end)
                split_point = max(last_period, last_newline)
                
                if split_point > start:
                    end = split_point + 1
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap
        
        return chunks
    
    def _analyze_json_structure(self, data: Any, max_depth: int = 3) -> Dict[str, Any]:
        """åˆ†æJSONç»“æ„"""
        if max_depth <= 0:
            return {"type": type(data).__name__}
        
        if isinstance(data, dict):
            return {
                "type": "dict",
                "keys": list(data.keys())[:10],  # æœ€å¤šæ˜¾ç¤º10ä¸ªé”®
                "key_count": len(data),
                "sample_values": {
                    k: self._analyze_json_structure(v, max_depth - 1)
                    for k, v in list(data.items())[:3]  # åˆ†æå‰3ä¸ªå€¼
                }
            }
        elif isinstance(data, list):
            return {
                "type": "list",
                "length": len(data),
                "sample_items": [
                    self._analyze_json_structure(item, max_depth - 1)
                    for item in data[:3]  # åˆ†æå‰3ä¸ªå…ƒç´ 
                ] if data else []
            }
        else:
            return {"type": type(data).__name__, "value": str(data)[:100]}
    
    def _extract_xml_text(self, element) -> str:
        """ä»XMLå…ƒç´ æå–æ–‡æœ¬"""
        text_parts = []
        
        if element.text:
            text_parts.append(element.text.strip())
        
        for child in element:
            text_parts.append(self._extract_xml_text(child))
            if child.tail:
                text_parts.append(child.tail.strip())
        
        return ' '.join(filter(None, text_parts))
    
    async def search_documents(
        self,
        query: str,
        collection_name: str = "walmart_documents",
        filters: Optional[Dict[str, Any]] = None,
        n_results: int = 10
    ) -> Dict[str, Any]:
        """æœç´¢æ–‡æ¡£"""
        if not self.vector_service:
            raise RuntimeError("å‘é‡æœåŠ¡æœªåˆå§‹åŒ–")
        
        return await self.vector_service.hybrid_search(
            collection_name=collection_name,
            query=query,
            filters=filters,
            n_results=n_results
        )
    
    async def get_document_stats(self, collection_name: str = "walmart_documents") -> Dict[str, Any]:
        """è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        if not self.vector_service:
            raise RuntimeError("å‘é‡æœåŠ¡æœªåˆå§‹åŒ–")
        
        return await self.vector_service.get_collection_stats(collection_name)
