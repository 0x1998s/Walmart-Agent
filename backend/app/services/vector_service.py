# ğŸ›’ æ²ƒå°”ç›AI Agentå¹³å° - å‘é‡æ•°æ®åº“æœåŠ¡
# Walmart AI Agent Platform - Vector Database Service

import asyncio
import logging
from typing import Dict, List, Optional, Tuple, Any
from uuid import uuid4

import chromadb
import numpy as np
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer

from app.core.config import get_settings

settings = get_settings()
logger = logging.getLogger(__name__)


class VectorService:
    """å‘é‡æ•°æ®åº“æœåŠ¡ - æ”¯æŒå¤šæºæ•°æ®æ•´åˆ"""
    
    def __init__(self):
        self.client: Optional[chromadb.Client] = None
        self.embedding_model: Optional[SentenceTransformer] = None
        self.collections: Dict[str, chromadb.Collection] = {}
        
    async def initialize(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“è¿æ¥"""
        try:
            # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯
            self.client = chromadb.HttpClient(
                host=settings.CHROMA_HOST,
                port=settings.CHROMA_PORT,
                settings=Settings(
                    chroma_client_auth_provider="chromadb.auth.basic_authn.BasicAuthClientProvider",
                    chroma_client_auth_credentials=f"{settings.CHROMA_AUTH_USER}:{settings.CHROMA_AUTH_PASSWORD}"
                )
            )
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            self.embedding_model = SentenceTransformer(settings.DEFAULT_EMBEDDING_MODEL)
            
            # åˆ›å»ºé»˜è®¤é›†åˆ
            await self._create_default_collections()
            
            logger.info("âœ… å‘é‡æ•°æ®åº“æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _create_default_collections(self):
        """åˆ›å»ºé»˜è®¤é›†åˆ"""
        default_collections = [
            "walmart_documents",      # æ²ƒå°”ç›æ–‡æ¡£é›†åˆ
            "product_catalog",        # å•†å“ç›®å½•
            "customer_data",          # å®¢æˆ·æ•°æ®
            "sales_reports",          # é”€å”®æŠ¥å‘Š
            "inventory_data",         # åº“å­˜æ•°æ®
            "market_analysis",        # å¸‚åœºåˆ†æ
            "operational_logs",       # è¿è¥æ—¥å¿—
        ]
        
        for collection_name in default_collections:
            try:
                collection = self.client.get_or_create_collection(
                    name=collection_name,
                    metadata={
                        "description": f"æ²ƒå°”ç›{collection_name}æ•°æ®é›†åˆ",
                        "created_by": "system",
                        "version": "1.0"
                    }
                )
                self.collections[collection_name] = collection
                logger.info(f"âœ… åˆ›å»ºé›†åˆ: {collection_name}")
                
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºé›†åˆå¤±è´¥ {collection_name}: {e}")
    
    async def add_documents(
        self,
        collection_name: str,
        documents: List[str],
        metadatas: Optional[List[Dict[str, Any]]] = None,
        ids: Optional[List[str]] = None,
    ) -> List[str]:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        try:
            if collection_name not in self.collections:
                self.collections[collection_name] = self.client.get_or_create_collection(collection_name)
            
            collection = self.collections[collection_name]
            
            # ç”ŸæˆID
            if ids is None:
                ids = [str(uuid4()) for _ in documents]
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            embeddings = await self._generate_embeddings(documents)
            
            # æ·»åŠ åˆ°é›†åˆ
            collection.add(
                documents=documents,
                embeddings=embeddings,
                metadatas=metadatas or [{} for _ in documents],
                ids=ids
            )
            
            logger.info(f"âœ… æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£åˆ°é›†åˆ {collection_name}")
            return ids
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    async def search_similar_documents(
        self,
        collection_name: str,
        query: str,
        n_results: int = 10,
        where: Optional[Dict[str, Any]] = None,
        include: List[str] = ["documents", "metadatas", "distances"]
    ) -> Dict[str, Any]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        try:
            if collection_name not in self.collections:
                raise ValueError(f"é›†åˆ {collection_name} ä¸å­˜åœ¨")
            
            collection = self.collections[collection_name]
            
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = await self._generate_embeddings([query])
            
            # æ‰§è¡Œæœç´¢
            results = collection.query(
                query_embeddings=query_embedding,
                n_results=n_results,
                where=where,
                include=include
            )
            
            logger.info(f"âœ… åœ¨é›†åˆ {collection_name} ä¸­æœç´¢åˆ° {len(results['ids'][0])} ä¸ªç›¸ä¼¼æ–‡æ¡£")
            return results
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    async def update_documents(
        self,
        collection_name: str,
        ids: List[str],
        documents: Optional[List[str]] = None,
        metadatas: Optional[List[Dict[str, Any]]] = None,
    ) -> bool:
        """æ›´æ–°æ–‡æ¡£"""
        try:
            if collection_name not in self.collections:
                raise ValueError(f"é›†åˆ {collection_name} ä¸å­˜åœ¨")
            
            collection = self.collections[collection_name]
            
            update_data = {"ids": ids}
            
            if documents:
                embeddings = await self._generate_embeddings(documents)
                update_data["documents"] = documents
                update_data["embeddings"] = embeddings
            
            if metadatas:
                update_data["metadatas"] = metadatas
            
            collection.update(**update_data)
            
            logger.info(f"âœ… æˆåŠŸæ›´æ–°é›†åˆ {collection_name} ä¸­çš„ {len(ids)} ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    async def delete_documents(
        self,
        collection_name: str,
        ids: List[str]
    ) -> bool:
        """åˆ é™¤æ–‡æ¡£"""
        try:
            if collection_name not in self.collections:
                raise ValueError(f"é›†åˆ {collection_name} ä¸å­˜åœ¨")
            
            collection = self.collections[collection_name]
            collection.delete(ids=ids)
            
            logger.info(f"âœ… æˆåŠŸåˆ é™¤é›†åˆ {collection_name} ä¸­çš„ {len(ids)} ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    async def get_collection_stats(self, collection_name: str) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            if collection_name not in self.collections:
                raise ValueError(f"é›†åˆ {collection_name} ä¸å­˜åœ¨")
            
            collection = self.collections[collection_name]
            count = collection.count()
            
            return {
                "name": collection_name,
                "count": count,
                "metadata": collection.metadata,
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–é›†åˆç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    async def list_collections(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰é›†åˆ"""
        try:
            collections = self.client.list_collections()
            return [
                {
                    "name": col.name,
                    "metadata": col.metadata,
                    "count": col.count()
                }
                for col in collections
            ]
            
        except Exception as e:
            logger.error(f"âŒ åˆ—å‡ºé›†åˆå¤±è´¥: {e}")
            return []
    
    async def create_collection(
        self,
        name: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> bool:
        """åˆ›å»ºæ–°é›†åˆ"""
        try:
            collection = self.client.create_collection(
                name=name,
                metadata=metadata or {}
            )
            self.collections[name] = collection
            
            logger.info(f"âœ… æˆåŠŸåˆ›å»ºé›†åˆ: {name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºé›†åˆå¤±è´¥ {name}: {e}")
            return False
    
    async def delete_collection(self, name: str) -> bool:
        """åˆ é™¤é›†åˆ"""
        try:
            self.client.delete_collection(name=name)
            if name in self.collections:
                del self.collections[name]
            
            logger.info(f"âœ… æˆåŠŸåˆ é™¤é›†åˆ: {name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤é›†åˆå¤±è´¥ {name}: {e}")
            return False
    
    async def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡"""
        if not self.embedding_model:
            raise RuntimeError("åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")
        
        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåµŒå…¥ç”Ÿæˆï¼ˆé¿å…é˜»å¡ï¼‰
        loop = asyncio.get_event_loop()
        embeddings = await loop.run_in_executor(
            None, 
            self.embedding_model.encode, 
            texts
        )
        
        return embeddings.tolist()
    
    async def hybrid_search(
        self,
        collection_name: str,
        query: str,
        filters: Optional[Dict[str, Any]] = None,
        n_results: int = 10,
        alpha: float = 0.7  # è¯­ä¹‰æœç´¢æƒé‡
    ) -> Dict[str, Any]:
        """æ··åˆæœç´¢ - ç»“åˆè¯­ä¹‰æœç´¢å’Œå…³é”®è¯æœç´¢"""
        try:
            # è¯­ä¹‰æœç´¢
            semantic_results = await self.search_similar_documents(
                collection_name=collection_name,
                query=query,
                n_results=n_results * 2,  # è·å–æ›´å¤šç»“æœç”¨äºé‡æ’åº
                where=filters
            )
            
            # å…³é”®è¯åŒ¹é…ï¼ˆç®€å•å®ç°ï¼‰
            keyword_scores = self._calculate_keyword_scores(
                query, 
                semantic_results.get("documents", [[]])[0]
            )
            
            # æ··åˆè¯„åˆ†
            final_results = self._combine_scores(
                semantic_results, 
                keyword_scores, 
                alpha, 
                n_results
            )
            
            return final_results
            
        except Exception as e:
            logger.error(f"âŒ æ··åˆæœç´¢å¤±è´¥: {e}")
            raise
    
    def _calculate_keyword_scores(self, query: str, documents: List[str]) -> List[float]:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        query_terms = set(query.lower().split())
        scores = []
        
        for doc in documents:
            doc_terms = set(doc.lower().split())
            intersection = query_terms.intersection(doc_terms)
            score = len(intersection) / len(query_terms) if query_terms else 0
            scores.append(score)
        
        return scores
    
    def _combine_scores(
        self,
        semantic_results: Dict[str, Any],
        keyword_scores: List[float],
        alpha: float,
        n_results: int
    ) -> Dict[str, Any]:
        """ç»„åˆè¯­ä¹‰å’Œå…³é”®è¯åˆ†æ•°"""
        if not semantic_results.get("distances"):
            return semantic_results
        
        distances = semantic_results["distances"][0]
        
        # å°†è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼‰
        max_distance = max(distances) if distances else 1
        semantic_scores = [(max_distance - d) / max_distance for d in distances]
        
        # ç»„åˆåˆ†æ•°
        combined_scores = [
            alpha * sem_score + (1 - alpha) * kw_score
            for sem_score, kw_score in zip(semantic_scores, keyword_scores)
        ]
        
        # æ ¹æ®ç»„åˆåˆ†æ•°æ’åº
        sorted_indices = sorted(
            range(len(combined_scores)),
            key=lambda i: combined_scores[i],
            reverse=True
        )[:n_results]
        
        # é‡æ–°ç»„ç»‡ç»“æœ
        result = {
            "ids": [[semantic_results["ids"][0][i] for i in sorted_indices]],
            "documents": [[semantic_results["documents"][0][i] for i in sorted_indices]],
            "metadatas": [[semantic_results["metadatas"][0][i] for i in sorted_indices]],
            "distances": [[1 - combined_scores[i] for i in sorted_indices]],  # è½¬å›è·ç¦»
        }
        
        return result
